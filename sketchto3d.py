# -*- coding: utf-8 -*-
"""sketchto3d.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18i931VEAfTGO5Eh7ePbR50Mr3llm9FIO
"""

import os
import argparse
import numpy as np
from PIL import Image

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models import vgg16_bn
from torch.utils.data import Dataset, DataLoader
from torch.optim import Adam

## Dataset Downloader
# Input - 2D sketch (png), Output - 3d voxel(.binvox) -> .npy
# Preprocess -> 2d point clouds --> 3d point cloud (using depth maps)

# Encoder
# Pixel - VGG16/19 -->
# Point cloud - PointNet/PointNet++ (3d Points) --> class

#Pixel-based features/ # Point-based features

# Fusion - MHSA + Cross attention

# Combined features -> 3D project

# Decoder - 3D conv --> Corase Voxel --> U-net --> Fine Voxel (Output)

import os
import numpy as np
import trimesh
import binvox_rw
from PIL import Image
import cv2

def load_binvox_as_npy(binvox_path):
    with open(binvox_path, 'rb') as f:
        model = binvox_rw.read_as_3d_array(f)
    return model.data.astype(np.uint8)

def project_depth_to_point_cloud(depth_map, intrinsic=np.eye(3)):
    h, w = depth_map.shape
    i, j = np.meshgrid(np.arange(w), np.arange(h))
    i = i.reshape(-1)
    j = j.reshape(-1)
    z = depth_map.reshape(-1)
    x = (i - intrinsic[0, 2]) * z / intrinsic[0, 0]
    y = (j - intrinsic[1, 2]) * z / intrinsic[1, 1]
    points = np.stack((x, y, z), axis=-1)
    return points[z > 0]  # filter out zero-depth

def sketch_to_pointcloud(sketch_path):
    sketch = Image.open(sketch_path).convert('L')
    sketch = sketch.resize((224, 224))
    sketch_np = np.array(sketch)

    # Generate pseudo-depth (simple inverse intensity or use depth estimator later)
    norm_img = sketch_np / 255.0
    depth = 1.0 - norm_img  # darker = closer

    depth = cv2.GaussianBlur(depth, (5, 5), 0)  # smoothing

    # Intrinsic camera matrix for projection (assumed)
    fx = fy = 1.0
    cx, cy = 112, 112
    intrinsic = np.array([[fx, 0, cx],
                          [0, fy, cy],
                          [0,  0,  1]])

    points = project_depth_to_point_cloud(depth, intrinsic)
    return points.astype(np.float32)

def preprocess_dataset(sketch_dir, voxel_dir, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    sketch_files = sorted(os.listdir(sketch_dir))

    for sketch_file in sketch_files:
        model_id = os.path.splitext(sketch_file)[0]

        sketch_path = os.path.join(sketch_dir, sketch_file)
        binvox_path = os.path.join(voxel_dir, model_id + ".binvox")

        if not os.path.exists(binvox_path):
            print(f"Missing voxel for: {model_id}")
            continue

        # Load 2D sketch to 3D point cloud
        point_cloud = sketch_to_pointcloud(sketch_path)

        # Load 3D voxel
        voxel = load_binvox_as_npy(binvox_path)

        # Save both
        np.save(os.path.join(output_dir, f"{model_id}_points.npy"), point_cloud)
        np.save(os.path.join(output_dir, f"{model_id}_voxel.npy"), voxel)

        print(f"Processed: {model_id}")

if __name__ == "__main__":
    preprocess_dataset(
        sketch_dir="path_to/sketches",         # folder with .png sketches
        voxel_dir="path_to/binvox_models",     # binvox files from ShapeNet
        output_dir="path_to/processed_output"  # saves .npy files here
    )

# Sketch image encoder
class SktEncoder(nn.Module):
    def __init__(self):
        super().__init__()

        vgg_model = vgg16_bn(pretrained=True)
        self.encoder = nn.Sequential(*vgg_model.features[:27])  # [B, 3, 224, 224] to [B, 512, 28, 28]

        self.custom = nn.Sequential(
            nn.Conv2d(512, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True)
        )

        self.pool = nn.MaxPool2d(kernel_size=3, stride=3)  # [B, 256, 28, 28] to [B, 256, 8, 8]

    def forward(self, x):
        #  [B, 1, 224, 224]
        if x.shape[1] == 1:
            x = x.repeat(1, 3, 1, 1)       # [B, 3, 224, 224]
        x = self.encoder(x)                # [B, 512, 28, 28]
        x = self.custom(x)                 # [B, 256, 28, 28]
        x = self.pool(x)                   # [B, 256, 8, 8]
        x = x.view(x.size(0), 256, -1)     # [B, 256, 64]
        return x                           # [B, 256, 64]


# Point Encoder
class SktPointEncoder(nn.Module):
    def __init__(self, num_points=256):
        super().__init__()
        self.num_points = num_points

        self.conv1 = nn.Sequential(
            nn.Conv1d(2, 64, 1),
            nn.BatchNorm1d(64),
            nn.ReLU(inplace=True)
        )

        self.conv2 = nn.Sequential(
            nn.Conv1d(2, 64, 3, padding=1),
            nn.BatchNorm1d(64),
            nn.ReLU(inplace=True)
        )

        self.conv3 = nn.Sequential(
            nn.Conv1d(2, 128, 5, padding=2),
            nn.BatchNorm1d(128),
            nn.ReLU(inplace=True)
        )

        # Combine features: 64 + 64 + 128 = 256
        self.combine_conv = nn.Sequential(
            nn.Conv1d(256, 1024, kernel_size=1),
            nn.BatchNorm1d(1024),
            nn.ReLU(inplace=True)
        )

        # MLP after concatenating local + global context (256 + 1024 = 1088)
        self.mlp = nn.Sequential(
            nn.Conv1d(1088, 512, kernel_size=1),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Conv1d(512, 256, kernel_size=1),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Conv1d(256, 64, kernel_size=1)
        )

    def forward(self, x):
        # x: [B, 256, 2], these are point coordinates
        x = x.permute(0, 2, 1)  # [B, 2, 256]

        f1 = self.conv1(x)      # [B, 64, 256]
        f3 = self.conv2(x)      # [B, 64, 256]
        f5 = self.conv3(x)      # [B, 128, 256]

        f_cat = torch.cat([f1, f3, f5], dim=1)  # [B, 256, 256]

        f_high = self.combine_conv(f_cat)  # [B, 1024, 256]

        global_feat = torch.max(f_high, dim=2, keepdim=True)[0]  # [B, 1024, 1]
        global_feat = global_feat.expand(-1, -1, self.num_points)  # [B, 1024, 256]

        fused = torch.cat([f1, global_feat], dim=1)  # [B, 64+1024=1088, 256]

        out = self.mlp(fused)  # [B, 64, 256]
        out = out.permute(0, 2, 1)  # [B, 256, 64]
        return out  # [B, 256, 64]


# Cross features fusion (self attention + cross)
class MHA(nn.Module):
    def __init__(self, embed_dim=512, num_heads=4, num_layers=6):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.MultiheadAttention(embed_dim, num_heads)
            for _ in range(num_layers)
        ])

    def forward(self, x):
        seq = x.permute(2, 0, 1)
        for layer in self.layers:
            seq, _ = layer(seq, seq, seq)
        return seq.permute(1, 2, 0)

# Criss Cross attention
class RCCA(nn.Module):
    def __init__(self, channels=512):
        super().__init__()
        self.q = nn.Conv1d(channels, channels, 1)
        self.k = nn.Conv1d(channels, channels, 1)
        self.v = nn.Conv1d(channels, channels, 1)
        self.softmax = nn.Softmax(dim=-1)
        self.channels = channels

    def forward(self, x):
        Q = self.q(x).permute(0,2,1)                # [B, L, C]
        K = self.k(x)                               # [B, C, L]
        attn = torch.matmul(Q, K) / (self.channels**0.5) # [B, L, L]
        attn = self.softmax(attn)
        V = self.v(x).permute(0,2,1)                # [B, L, C]
        context = torch.matmul(attn, V).permute(0,2,1) # [B, C, L]
        return x + context                          # [B, C, L]

# Fusion together
class CFAM(nn.Module):
    def __init__(self):
        super().__init__()
        self.rcca = RCCA(channels=512)
        self.mha  = MHA(embed_dim=512, num_heads=4, num_layers=6)

    def forward(self, Ft, Fp):
        Ft = Ft.view(Ft.size(0), 256, -1)       # ensure [B,256,64]
        Fp = Fp.view(Fp.size(0), 256, -1)       # [B,256,64]
        fin = torch.cat([Ft, Fp], dim=1)        # [B,512,64]
        out = self.rcca(fin)                    # [B,512,64]
        out = self.mha(out)                     # [B,512,64]
        return out                              # [B,512,64]

# Coarse Decoder 3D
class Decoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(512 * 64, 512 * 4 * 4 * 4)
        self.deconv = nn.Sequential(
            nn.ConvTranspose3d(512, 512, 4, stride=2, padding=1),
            nn.BatchNorm3d(512), nn.ReLU(inplace=True),
            nn.ConvTranspose3d(512, 128, 4, stride=2, padding=1),
            nn.BatchNorm3d(128), nn.ReLU(inplace=True),
            nn.ConvTranspose3d(128, 32, 4, stride=2, padding=1),
            nn.BatchNorm3d(32), nn.ReLU(inplace=True),
            nn.ConvTranspose3d(32, 8, 4, stride=2, padding=1),
            nn.BatchNorm3d(8), nn.ReLU(inplace=True),
            nn.ConvTranspose3d(8, 1, 1, stride=1, padding=0),
            nn.Sigmoid()
        )

    def forward(self, x):
        B = x.size(0)
        v = x.view(B, -1)
        v = self.fc(v)
        v = v.view(B, 512, 4, 4, 4)
        return self.deconv(v)


# 3D U-Net–style Refiner
class Refiner(nn.Module):
    def __init__(self):
        super().__init__()
        self.enc1 = nn.Sequential(
            nn.Conv3d(1, 32, kernel_size=4, padding=2),
            nn.BatchNorm3d(32), nn.LeakyReLU(0.2, inplace=True),
            nn.MaxPool3d(2)  # 32→16
        )
        self.enc2 = nn.Sequential(
            nn.Conv3d(32, 64, kernel_size=4, padding=2),
            nn.BatchNorm3d(64), nn.LeakyReLU(0.2, inplace=True),
            nn.MaxPool3d(2)  # 16→8
        )
        self.enc3 = nn.Sequential(
            nn.Conv3d(64, 128, kernel_size=4, padding=2),
            nn.BatchNorm3d(128), nn.LeakyReLU(0.2, inplace=True),
            nn.MaxPool3d(2)  # 8→4
        )

        # Semantic FC‐bottleneck
        self.fc_sem = nn.Sequential(
            nn.Linear(128 * 4 * 4 * 4, 2048),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(2048, 8192),
            nn.LeakyReLU(0.2, inplace=True)
        )

        self.up1 = nn.Sequential(
            nn.ConvTranspose3d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm3d(64), nn.ReLU(inplace=True)  # 4→8
        )
        self.up2 = nn.Sequential(
            nn.ConvTranspose3d(128, 32, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm3d(32), nn.ReLU(inplace=True)  # 8→16
        )
        self.up3 = nn.Sequential(
            nn.ConvTranspose3d(64, 1, kernel_size=4, stride=2, padding=1),
            nn.Sigmoid()  # 16→32
        )

    def forward(self, x_coarse):
        # x_coarse: [B,1,32,32,32]
        e1 = self.enc1(x_coarse)            # [B,32,16,16,16]
        e2 = self.enc2(e1)                  # [B,64, 8, 8, 8]
        e3 = self.enc3(e2)                  # [B,128,4, 4, 4]

        B = x_coarse.size(0)
        sem = e3.view(B, -1)                # [B,128*4*4*4]
        sem = self.fc_sem(sem)              # [B,8192]
        sem = sem.view(B, 128, 4, 4, 4)     # [B,128,4,4,4]

        x = e3 + sem                        # [B,128,4,4,4]

        d1 = self.up1(x)                    # [B,64, 8, 8, 8]
        d1 = torch.cat([d1, e2], dim=1)     # [B,64+64=128,8,8,8]

        d2 = self.up2(d1)                   # [B,32,16,16,16]
        d2 = torch.cat([d2, e1], dim=1)     # [B,32+32=64,16,16,16]

        return self.up3(d2)                 # [B,1,32,32,32]

class SketchDataset(Dataset):
    def __init__(self, samples, transform=None):
        self.samples  = samples
        self.transform = transform

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        rec = self.samples[idx]
        pts   = load_points(rec['points'])
        vox   = load_voxel(rec['voxel'])
        if self.transform:
            depth = self.transform(depth)
        return pts, vox

class CSE252Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.img_enc   = SktEncoder()
        self.point_enc = SktPointEncoder()
        self.fusion    = CFAM()
        self.decoder   = Decoder()
        self.refiner   = Refiner()

    def forward(self, img, pts):
        img_features   = self.img_enc(img)
        pts_features   = self.point_enc(pts)
        features = self.fusion(img_features, pts_features)
        coarse_vol  = self.decoder(features)
        refined_vol = self.refiner(coarse_vol)
        return coarse_vol, refined_vol


def train(
    train_loader,
    val_loader=None,
    batch_size=8,
    lr=1e-4,
    epochs=50,
    device='cuda',
):